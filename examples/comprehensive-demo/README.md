# Litechain Comprehensive Demo

This project demonstrates all the key features of Litechain, including:

## Features Demonstrated

### ðŸ”— **LLM Chaining & Routing**
- Automatic transfer and escalation between LLMs
- Multi-department customer support simulation
- Connection management and routing rules

### ðŸ§  **Memory Management**
- Chat memory for conversation history
- Vector memory for semantic search
- Hybrid memory combining both approaches
- Persistent storage across sessions

### âš¡ **Streaming Support**
- Real-time streaming responses
- Chunk handling and processing
- Stream management utilities

### ðŸ› ï¸ **Advanced Tool Integration**
- Multiple tools with different parameter types
- Tool chaining and composition
- Error handling and validation

### ðŸ” **State Management & Debugging**
- Conversation flow tracking
- Transfer history monitoring
- Debug utilities and introspection

## Setup

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Environment setup:**
   Create a `.env` file with your API keys:
   ```env
   OPENAI_API_KEY=your_openai_key_here
   GROQ_API_KEY=your_groq_key_here
   GEMINI_API_KEY=your_gemini_key_here
   CLAUDE_API_KEY=your_claude_key_here
   ```

## Running the Demos

### Individual Feature Demos

```bash
# Basic usage patterns
npm run demo:basic

# LLM chaining and routing
npm run demo:chaining

# Memory management features
npm run demo:memory

# Streaming capabilities
npm run demo:streaming

# Advanced tool integration
npm run demo:tools
```

### Comprehensive Demo

```bash
# Run all features in an interactive demo
npm run demo:all
```

## Project Structure

```
comprehensive-demo/
â”œâ”€â”€ package.json
â”œâ”€â”€ README.md
â”œâ”€â”€ .env.example
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ basic-usage.ts          # Basic Litechain usage
â”œâ”€â”€ llm-chaining.ts         # LLM chaining and routing
â”œâ”€â”€ memory-demo.ts          # Memory management
â”œâ”€â”€ streaming-demo.ts       # Streaming features
â”œâ”€â”€ advanced-tools.ts       # Advanced tool usage
â”œâ”€â”€ comprehensive-demo.ts   # Interactive comprehensive demo
â””â”€â”€ utils/
    â”œâ”€â”€ demo-helpers.ts     # Demo utility functions
    â”œâ”€â”€ mock-data.ts        # Mock data for demos
    â””â”€â”€ logger.ts           # Enhanced logging
```

## Key Concepts

### LLM Chaining
Litechain allows you to chain multiple LLMs together with automatic routing:

```typescript
// Set up LLMs with routing rules
entryLLM.systemprompt = "For billing issues, respond with: [TRANSFER:BILLING]";
entryLLM.connect({ BILLING: billingLLM });

// Automatic routing happens based on LLM responses
const response = await entryLLM.invoke("I want a refund");
```

### Memory Integration
Multiple memory types for different use cases:

```typescript
// Chat memory for conversation history
const chatLLM = litechain.llm.openai({ 
  apiKey: "...", 
  model: "gpt-4",
  memoryConfig: 'chat' 
});

// Vector memory for semantic search
const vectorLLM = litechain.llm.openai({ 
  apiKey: "...", 
  model: "gpt-4",
  memoryConfig: 'vector' 
});
```

### Streaming Support
Built-in streaming with easy setup:

```typescript
const response = await llm.stream("Tell me a story", {
  onChunk: (chunk) => process.stdout.write(chunk.delta),
  onComplete: (content) => console.log('\nComplete!')
});
```

## Best Practices

1. **Error Handling**: Always handle potential errors in LLM responses
2. **Memory Management**: Choose appropriate memory types for your use case
3. **Tool Design**: Keep tools focused and well-documented
4. **State Monitoring**: Use built-in debugging tools for development

## Next Steps

After running the demos, explore:
- Custom memory implementations
- Advanced tool compositions
- Production deployment patterns
- Performance optimization techniques
